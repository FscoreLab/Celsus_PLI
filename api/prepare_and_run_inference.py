#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ DICOM —Ñ–∞–π–ª–æ–≤ –∏ –∑–∞–ø—É—Å–∫–∞ inference —á–µ—Ä–µ–∑ API.

–ë–µ—Ä—ë—Ç —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–Ω—ã–µ DICOM —Ñ–∞–π–ª—ã, –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏—Ö –ø–æ StudyInstanceUID,
–∑–∞–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç –≤ ZIP –∞—Ä—Ö–∏–≤—ã –∏ –ø—Ä–æ–≥–æ–Ω—è–µ—Ç —á–µ—Ä–µ–∑ API.
"""

import argparse
import os
import sys
import tempfile
import zipfile
from pathlib import Path

import pandas as pd
import pydicom
from tqdm import tqdm


def group_dicom_files_by_study(input_dir: Path) -> dict:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç DICOM —Ñ–∞–π–ª—ã –ø–æ StudyInstanceUID."""
    print(f"\nüìÇ –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {input_dir}")

    studies = {}
    skipped = 0

    all_files = []
    for root, _, files in os.walk(input_dir):
        for file in files:
            if file.startswith(".") or file.endswith((".txt", ".pdf", ".jpg", ".png")):
                continue
            all_files.append(Path(root) / file)

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_files)} —Ñ–∞–π–ª–æ–≤")

    for file_path in tqdm(all_files, desc="–ß—Ç–µ–Ω–∏–µ DICOM —Ñ–∞–π–ª–æ–≤"):
        try:
            dcm = pydicom.dcmread(file_path, stop_before_pixels=True)

            study_uid = str(dcm.StudyInstanceUID)

            path_str = str(file_path)
            if "norma" in path_str.lower():
                category = "norma"
            elif "pneumonia" in path_str.lower():
                category = "pneumonia"
            elif "pneumotorax" in path_str.lower():
                category = "pneumotorax"
            else:
                category = "unknown"

            if study_uid not in studies:
                studies[study_uid] = {"files": [], "category": category}

            studies[study_uid]["files"].append(file_path)

        except Exception as e:
            skipped += 1
            continue

    print(f"\n‚úÖ –°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–æ {len(studies)} –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π")
    print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ {skipped} —Ñ–∞–π–ª–æ–≤ (–Ω–µ DICOM)")

    categories = {}
    for study_info in studies.values():
        cat = study_info["category"]
        categories[cat] = categories.get(cat, 0) + 1

    print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for cat, count in sorted(categories.items()):
        print(f"  {cat}: {count}")

    return studies


def create_zip_archives(studies: dict, output_dir: Path) -> list:
    """–°–æ–∑–¥–∞—ë—Ç ZIP –∞—Ä—Ö–∏–≤—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    output_dir.mkdir(parents=True, exist_ok=True)
    archives = []

    print(f"\nüì¶ –°–æ–∑–¥–∞—ë–º ZIP –∞—Ä—Ö–∏–≤—ã –≤: {output_dir}")

    for study_uid, study_info in tqdm(studies.items(), desc="–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–æ–≤"):
        category = study_info["category"]
        files = study_info["files"]

        safe_uid = study_uid.replace(".", "_")
        zip_name = f"{category}_{safe_uid}.zip"
        zip_path = output_dir / zip_name

        try:
            with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
                for file_path in files:
                    zf.write(file_path, file_path.name)

            archives.append(
                {"path": zip_path, "study_uid": study_uid, "category": category, "num_files": len(files)}
            )

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞—Ä—Ö–∏–≤–∞ {zip_name}: {e}")
            continue

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(archives)} –∞—Ä—Ö–∏–≤–æ–≤")
    return archives


def run_batch_inference(archives: list, api_url: str, output_excel: Path):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç batch inference —á–µ—Ä–µ–∑ API."""
    import requests

    print(f"\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º batch inference...")
    print(f"API URL: {api_url}")

    try:
        response = requests.get(f"{api_url}/health", timeout=10)
        if response.status_code == 200:
            print("‚úÖ API –¥–æ—Å—Ç—É–ø–µ–Ω")
        else:
            print(f"‚ùå API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (HTTP {response.status_code})")
            return
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API: {e}")
        return

    results = []

    for archive_info in tqdm(archives, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤"):
        archive_path = archive_info["path"]

        try:
            with open(archive_path, "rb") as f:
                files = {"file": (archive_path.name, f, "application/zip")}
                response = requests.post(f"{api_url}/predict", files=files, timeout=600)

            if response.status_code == 200:
                result = response.json()
                result["path_to_study"] = str(archive_path)
                result["ground_truth_category"] = archive_info["category"]
                result["num_files"] = archive_info["num_files"]
                results.append(result)

                status = "‚úÖ" if result["pathology"] == "–ü–∞—Ç–æ–ª–æ–≥–∏—è" else "‚ö™"
                tqdm.write(
                    f"{status} {archive_path.name}: {result['pathology']} "
                    f"(P={result['probability_of_pathology']:.3f}, GT={archive_info['category']})"
                )

            else:
                error_data = response.json() if response.headers.get("content-type") == "application/json" else {}
                results.append(
                    {
                        "path_to_study": str(archive_path),
                        "study_uid": archive_info["study_uid"],
                        "series_uid": "",
                        "probability_of_pathology": 0.0,
                        "pathology": "Unknown",
                        "most_dangerous_pathology_type": "Unknown",
                        "processing_status": "Failure",
                        "time_of_processing": 0.0,
                        "ground_truth_category": archive_info["category"],
                        "num_files": archive_info["num_files"],
                        "error": error_data.get("error", f"HTTP {response.status_code}"),
                    }
                )

        except Exception as e:
            results.append(
                {
                    "path_to_study": str(archive_path),
                    "study_uid": archive_info["study_uid"],
                    "series_uid": "",
                    "probability_of_pathology": 0.0,
                    "pathology": "Unknown",
                    "most_dangerous_pathology_type": "Unknown",
                    "processing_status": "Failure",
                    "time_of_processing": 0.0,
                    "ground_truth_category": archive_info["category"],
                    "num_files": archive_info["num_files"],
                    "error": str(e),
                }
            )

    df = pd.DataFrame(results)

    columns_order = [
        "path_to_study",
        "ground_truth_category",
        "study_uid",
        "series_uid",
        "probability_of_pathology",
        "pathology",
        "most_dangerous_pathology_type",
        "processing_status",
        "time_of_processing",
        "num_files",
    ]
    columns_order = [col for col in columns_order if col in df.columns]
    other_cols = [col for col in df.columns if col not in columns_order]
    df = df[columns_order + other_cols]

    df.to_excel(output_excel, index=False, engine="openpyxl")
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_excel}")

    print("\n" + "=" * 80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)

    total = len(results)
    success = sum(1 for r in results if r["processing_status"] == "Success")
    print(f"–í—Å–µ–≥–æ: {total}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success} ({success/total*100:.1f}%)")
    print(f"‚ùå –û—à–∏–±–∫–∏: {total - success} ({(total-success)/total*100:.1f}%)")

    if success > 0:
        success_df = df[df["processing_status"] == "Success"]

        print("\n–ü–æ ground truth –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for cat in ["norma", "pneumonia", "pneumotorax"]:
            cat_df = success_df[success_df["ground_truth_category"] == cat]
            if len(cat_df) > 0:
                pathology_count = (cat_df["pathology"] == "–ü–∞—Ç–æ–ª–æ–≥–∏—è").sum()
                print(f"  {cat}: {len(cat_df)} –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, {pathology_count} –ø–æ–º–µ—á–µ–Ω–æ –∫–∞–∫ –ø–∞—Ç–æ–ª–æ–≥–∏—è")

        if "most_dangerous_pathology_type" in success_df.columns:
            top_pathologies = success_df["most_dangerous_pathology_type"].value_counts().head(5)
            print("\n–¢–æ–ø-5 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –æ–ø–∞—Å–Ω—ã—Ö –ø–∞—Ç–æ–ª–æ–≥–∏–π:")
            for i, (path, count) in enumerate(top_pathologies.items(), 1):
                print(f"  {i}. {path}: {count}")


def main():
    parser = argparse.ArgumentParser(description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø—É—Å–∫ inference")
    parser.add_argument(
        "--input-dir",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–Ω—ã–º–∏ DICOM —Ñ–∞–π–ª–∞–º–∏",
    )
    parser.add_argument(
        "--temp-dir", type=str, default=None, help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö ZIP –∞—Ä—Ö–∏–≤–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: /tmp/...)"
    )
    parser.add_argument("--api-url", type=str, default="http://localhost:8000", help="URL API —Å–µ—Ä–≤–∏—Å–∞")
    parser.add_argument("--output", type=str, required=True, help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Excel —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
    parser.add_argument(
        "--skip-zip-creation",
        action="store_true",
        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–æ–≤ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)",
    )

    args = parser.parse_args()

    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_dir}")
        return 1

    if args.temp_dir:
        temp_dir = Path(args.temp_dir)
    else:
        temp_dir = Path(tempfile.gettempdir()) / "ct_clip_inference_archives"

    output_excel = Path(args.output)

    print("=" * 80)
    print("CT-CLIP + LightGBM Inference Pipeline")
    print("=" * 80)

    if not args.skip_zip_creation:
        studies = group_dicom_files_by_study(input_dir)
        archives = create_zip_archives(studies, temp_dir)
    else:
        print(f"\nüì¶ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ö–∏–≤—ã –∏–∑: {temp_dir}")
        archives = []
        for zip_file in temp_dir.glob("*.zip"):
            parts = zip_file.stem.split("_", 1)
            category = parts[0] if len(parts) > 1 else "unknown"
            study_uid = parts[1].replace("_", ".") if len(parts) > 1 else zip_file.stem

            archives.append(
                {"path": zip_file, "study_uid": study_uid, "category": category, "num_files": 0}
            )
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(archives)} –∞—Ä—Ö–∏–≤–æ–≤")

    if not archives:
        print("‚ùå –ù–µ—Ç –∞—Ä—Ö–∏–≤–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return 1

    run_batch_inference(archives, args.api_url, output_excel)

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    return 0


if __name__ == "__main__":
    sys.exit(main())
